---
title: 'CASE Workshop 2025: Shortcourse'
output:
  pdf_document: default
  html_document: default
date: "2025-06-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
We will be using Google Colab to implement this deep extreme quantile regression example. First, go to [https://colab.research.google.com/](https://colab.research.google.com/). Click File -> New notebook in Drive, and then change the runtime to R (Runtime -> Change runtime type, then pick R in the dropdown). We will not be using GPUs, so keep the CPU box checked.

```{r, include = F}
library(keras)
reticulate::use_condaenv("USWildfiresExtremes", required = T)
sess = k_get_session()
sess$list_devices()
library(tensorflow)
```

# Installation

To install Keras, run the following code. Colab already has Python and Tensorflow modules installed, so we do not need to do anything particularly complicated here. If installing Keras for R on your own machine, see the tutorial here [https://tensorflow.rstudio.com/install/](https://tensorflow.rstudio.com/install/). We also have a short tutorial here: [https://github.com/callumbarltrop/DeepGaugePublic](https://github.com/callumbarltrop/DeepGaugePublic/blob/main/README.md).

```{r, eval = F}
remotes::install_github("rstudio/tensorflow")
```

```{r eval = F}
install.packages(c("keras","lime","evd"))
```

Now, check to see if Keras has installed correctly.
```{r }
library("keras")
is_keras_available()
```

Set seeds for reproducibility.
```{r}
set.seed(1)
tensorflow::set_random_seed(1)
```

## Simulating training data

We build a neural estiamtor for the posterior mean of a bivariate Husler-Reiss distribution.

First, set a prior on the dependence parameter, here denoted by $\alpha$.
```{r}
prior_alpha = function(K) {
  rgamma(K , 1, 1)
}
```

Now, generate some training data. We use $n=64$ replicates as this will coincide with the number of observations we have.
```{r}
K <- 50000
n <- 64
alpha.train <- prior_alpha(K)

Z.train <- t(apply(as.matrix(alpha.train), 1, function(x)
  c(evd::rbvevd(
    n, dep = x, model = "hr"
  ))))

```


## Build estimator
```{r}
model <- keras_model_sequential()

model %>%
  # Adds a densely-connected layer with 64 units to the model:
  layer_dense(units = 64, activation = 'relu') %>%
  
  # Add another:
  layer_dense(units = 64, activation = 'relu') %>%
  
  
  # Add a final layer with 1 ouput
  layer_dense(units = 1, activation = 'exponential')
```

Now, compile the model with a loss function and an optimizer. Here we use Adam with standard hyper-parameters, and the MSE loss function to target the posterior mean.

```{r}
model %>% compile(
  optimizer = "adam",
  loss = "mean_squared_error"
)
```


Now fit the model. We train the model for 100 epochs, with an 80/20 validation data split. The default minibatch size is 16. Note that the model gradients are not evaluated on the validation data, and instead we can use the validation loss (i.e., the loss evaluated on the validation data) to motivate hyperparameters (e.g., neural net architecture) choices.

If you choose to cancel training (ctrl + C, or the big red stop button), then the current model state will be saved and accessible. Feel free to set `verbose = 1` to track the training.

```{r}
early.stopping <-   callback_early_stopping(monitor = "val_loss", patience = 10)

history <- model %>% fit(
  x = Z.train,
  y = as.matrix(alpha.train),
  callbacks = list(early.stopping),
  epochs = 100,
  verbose = 0,
  validation_split = 0.2,
  shuffle = T
)
```
Plot the training history, and print the summary of the architecture.
```{r}
plot(history)
summary(model)
```
Now, let's see how well the estimator performs. We generate 1000 test datasets and compare the true values of $\alpha$ with the predictions.

```{r}
K.test <- 1000
alpha.test <- prior_alpha(K.test)

Z.test <- t(apply(as.matrix(alpha.test), 1, function(x)
  c(evd::rbvevd(
    n, dep = x, model = "hr"
  ))))

predictions <- model %>% predict(Z.test)

plot(alpha.test, predictions)
abline(a = 0, b = 1)
```

# Get annual maxima

Now, we apply the estimator to real data. We will analyse annual temperature maxima.

Extract the maxima from two sites.

```{r}
data <- readRDS("weather.RDS")
data = data[data$loc == 1 | data$loc == 2, ]
years = unique(format(as.Date(data$time), "%Y"))

int <- 1
tmax <- matrix(0, nrow = length(years), ncol = 2)

for (t in years) {
  inds = which(format(as.Date(data$time), "%Y") == t & data$loc == 1)
  tmax[int, 1] = max(data$tmax[inds])
  
  inds = which(format(as.Date(data$time), "%Y") == t &
                 data$loc == 2)
  tmax[int, 2] = max(data$tmax[inds])
  
  int = int + 1
}
```

Here we see strong asymptotic dependence in the bivariate temperature maxima.
```{r}
plot(tmax)
```

We first transform the data onto standard Gumbel margins, as this was the same marginal scale used for generating the training data.
```{r}
unif = function(x)
  rank(x) / (length(x) + 1)

tmax.Gumbel = evd::qgev(apply(tmax, 2, unif))
plot(tmax.Gumbel)
```

Get your estimate for the dataset.
```{r}
alpha.est <- model %>% predict(t(as.matrix(c(tmax.Gumbel))))
```

We can visualise the model fit using density contours. The model appears to fit the data quite well.
```{r}
plot(tmax.Gumbel)
sims <- evd::rbvevd(100000, alpha.est, model = "hr")
biv_kde <- MASS::kde2d(sims[, 1], sims[, 2])
contour(biv_kde, add = T, col = "red")
```

The estimator is amortised, so can be reapplied to new data without retraining. To illustrate this, we generate a new dataset with locations further apart; see now that the data appear to exhibit asymptotic independence

```{r}
data <- readRDS("weather.RDS")
data = data[data$loc == 1 | data$loc == 25, ]
years = unique(format(as.Date(data$time), "%Y"))

int <- 1
tmax <- matrix(0, nrow = length(years), ncol = 2)

for (t in years) {
  inds = which(format(as.Date(data$time), "%Y") == t & data$loc == 1)
  tmax[int, 1] = max(data$tmax[inds])
  
  inds = which(format(as.Date(data$time), "%Y") == t &
                 data$loc == 25)
  tmax[int, 2] = max(data$tmax[inds])
  
  int = int + 1
}

tmax.Gumbel = evd::qgev(apply(tmax, 2, unif))
plot(tmax.Gumbel)
```

Let's get the parameter estimate.
```{r}
alpha.est <- model %>% predict(t(as.matrix(c(tmax.Gumbel))))
```

```{r}
plot(tmax.Gumbel)
sims <- evd::rbvevd(100000, alpha.est, model="hr")
biv_kde <- MASS::kde2d(sims[,1], sims[,2])
contour(biv_kde, add = T,col="red")
```

The estimator here is specific to a dataset with $n=64$ replicates, but we can build estimators that are applicable to general sample sizes; see, e.g., the [NeuralEstimators](https://cran.r-project.org/web/packages/NeuralEstimators/vignettes/NeuralEstimators.html) package.
